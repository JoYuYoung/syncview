# syncview_ai_service/main.py
# Cloud Runì—ì„œ ì‹¤í–‰ë  AI ì „ìš© ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import pipeline
import logging
import os
import gc

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë¡œê¹… ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (CPU ê°•ì œ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["ACCELERATE_USE_CPU"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# FastAPI ì•± ìƒì„±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
app = FastAPI(title="SyncView AI Service", version="1.0.0")

# CORS ì„¤ì • (ëª¨ë“  origin í—ˆìš© - í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ë¡œë”©)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
sentiment_analyzer = None
summarizer = None
translator = None

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Pydantic ëª¨ë¸ (ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class SentimentRequest(BaseModel):
    text: str

class SummarizeRequest(BaseModel):
    text: str
    max_length: int = 130
    min_length: int = 30

class TranslateRequest(BaseModel):
    text: str
    source_lang: str = "en"
    target_lang: str = "ko"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ (ëª¨ë¸ ì‚¬ì „ ë¡œë”©)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ (ê°ì„± ë¶„ì„ë§Œ ì‚¬ì „ ë¡œë”©, ë‚˜ë¨¸ì§€ëŠ” ì§€ì—° ë¡œë”©)"""
    global sentiment_analyzer
    
    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info("ğŸš€ SyncView AI Service ì‹œì‘ (ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë“œ)")
    logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    try:
        # âœ… ê°ì„± ë¶„ì„ë§Œ ì‚¬ì „ ë¡œë”© (ê°€ì¥ ë§ì´ ì‚¬ìš©)
        logger.info("ğŸ”„ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=-1,
            framework="pt"
        )
        gc.collect()
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("ğŸ‰ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
        logger.info("ğŸ’¡ ìš”ì•½/ë²ˆì—­ ëª¨ë¸ì€ ì²« ìš”ì²­ ì‹œ ìë™ ë¡œë“œ (10-20ì´ˆ)")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# API ì—”ë“œí¬ì¸íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.get("/")
def root():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "SyncView AI Service",
        "status": "running",
        "version": "1.0.0",
        "models": {
            "sentiment": "loaded" if sentiment_analyzer else "not loaded",
            "summarize": "loaded" if summarizer else "not loaded",
            "translate": "loaded" if translator else "not loaded"
        }
    }

@app.get("/health")
def health():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "models_loaded": all([sentiment_analyzer, summarizer, translator])
    }

@app.post("/sentiment")
def analyze_sentiment(request: SentimentRequest):
    """ê°ì„± ë¶„ì„ API"""
    try:
        if not sentiment_analyzer:
            raise HTTPException(status_code=503, detail="ê°ì„± ë¶„ì„ ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        text = request.text[:512]  # ìµœëŒ€ 512 í† í°
        result = sentiment_analyzer(text)[0]
        
        # POSITIVE/NEGATIVEë¥¼ í•œêµ­ì–´ë¡œ ë³€í™˜
        sentiment_map = {
            "POSITIVE": {"sentiment": "positive", "label": "ê¸ì •"},
            "NEGATIVE": {"sentiment": "negative", "label": "ë¶€ì •"}
        }
        
        sentiment_info = sentiment_map.get(result["label"], {"sentiment": "neutral", "label": "ì¤‘ë¦½"})
        
        return {
            "sentiment": sentiment_info["sentiment"],
            "label": sentiment_info["label"],
            "score": round(result["score"], 2)
        }
        
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/summarize")
def summarize_text(request: SummarizeRequest):
    """í…ìŠ¤íŠ¸ ìš”ì•½ API (ì§€ì—° ë¡œë”©)"""
    global summarizer
    
    try:
        # ì§€ì—° ë¡œë”©: ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ
        if not summarizer:
            logger.info("ğŸ”„ ìš”ì•½ ëª¨ë¸ ì§€ì—° ë¡œë”© ì¤‘... (ì²« ìš”ì²­)")
            summarizer = pipeline(
                "summarization",
                model="sshleifer/distilbart-cnn-12-6",
                device=-1,
                framework="pt"
            )
            gc.collect()
            logger.info("âœ… ìš”ì•½ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        text = request.text[:1024]  # ìµœëŒ€ 1024ì
        
        if len(text.strip()) < 50:
            return {"summary": "í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        result = summarizer(
            text,
            max_length=request.max_length,
            min_length=request.min_length,
            do_sample=False
        )
        
        return {"summary": result[0]["summary_text"]}
        
    except Exception as e:
        logger.error(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}")

@app.post("/translate")
def translate_text(request: TranslateRequest):
    """í…ìŠ¤íŠ¸ ë²ˆì—­ API (ì˜ì–´ â†’ í•œêµ­ì–´, ì§€ì—° ë¡œë”©)"""
    global translator
    
    try:
        # ì§€ì—° ë¡œë”©: ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ
        if not translator:
            logger.info("ğŸ”„ ë²ˆì—­ ëª¨ë¸ ì§€ì—° ë¡œë”© ì¤‘... (ì²« ìš”ì²­)")
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            model_name = "facebook/nllb-200-distilled-600M"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            translator = {"tokenizer": tokenizer, "model": model}
            gc.collect()
            logger.info("âœ… ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        tokenizer = translator["tokenizer"]
        model = translator["model"]
        
        # NLLB ëª¨ë¸ì€ ì†ŒìŠ¤/íƒ€ê¹ƒ ì–¸ì–´ ì„¤ì • í•„ìš”
        tokenizer.src_lang = "eng_Latn"  # ì˜ì–´
        
        # í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (700ìì”©)
        chunks = []
        text = request.text
        while text:
            chunk = text[:700]
            chunks.append(chunk)
            text = text[700:]
        
        # ê° ì²­í¬ ë²ˆì—­
        translated_chunks = []
        for chunk in chunks:
            inputs = tokenizer(chunk, return_tensors="pt", padding=True, truncation=True, max_length=512)
            
            # NLLBëŠ” forced_bos_token_idë¡œ íƒ€ê¹ƒ ì–¸ì–´ ì§€ì •
            forced_bos_token_id = tokenizer.convert_tokens_to_ids("kor_Hang")  # í•œêµ­ì–´
            
            with torch.no_grad():
                generated_tokens = model.generate(
                    **inputs,
                    forced_bos_token_id=forced_bos_token_id,
                    num_beams=4,
                    max_length=512,
                    early_stopping=True
                )
            
            translated = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
            translated_chunks.append(translated)
        
        return {"translated_text": " ".join(translated_chunks)}
        
    except Exception as e:
        logger.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„œë²„ ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)

