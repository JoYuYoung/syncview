from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
import feedparser
import requests
from bs4 import BeautifulSoup
import logging
from typing import List, Dict, Any, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import random
from datetime import datetime
from dateutil import parser as date_parser
from sqlalchemy.orm import Session
from database import get_db
from models import ReadArticle
from urllib.parse import urlparse
from utils import call_ai_service


router = APIRouter()
logger = logging.getLogger(__name__)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# (ê¸°ì¡´) ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ìœ í‹¸ - ì§€ê¸ˆì€ ì§ì ‘ í˜¸ì¶œí•˜ì§„ ì•Šì§€ë§Œ ë‚¨ê²¨ë‘ 
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def extract_reuters_article(soup: BeautifulSoup) -> str:
    """
    Reuters ì „ìš© ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
    - <article> ë˜ëŠ” data-testid="BodyWrapper" ê°™ì€ ì»¨í…Œì´ë„ˆ ì•ˆì˜ <p>ë¥¼ ëª¨ìŒ
    - 'Subscribe to Reuters' ê°™ì€ êµ¬ë… ë¬¸êµ¬ ì•„ë˜ëŠ” ë²„ë¦¼
    """
    article = (
        soup.find("div", attrs={"data-testid": "Body"})
        or soup.find("article")
        or soup.find(attrs={"data-testid": "BodyWrapper"})
        or soup.body
    )

    paragraphs = []
    for p in article.find_all("p", recursive=True):
        text = p.get_text(" ", strip=True)
        if not text:
            continue

        lower = text.lower()
        # êµ¬ë…/ì•½ê´€/ê´‘ê³  ê°™ì€ í•˜ë‹¨ ë¬¸êµ¬ ë§Œë‚˜ë©´ ê±°ê¸°ì„œ ëŠê¸°
        if "subscribe to reuters" in lower:
            break
        if "our standards:" in lower and "thomson reuters trust principles" in lower:
            break

        paragraphs.append(text)

    return "\n\n".join(paragraphs).strip()


def extract_generic_article(soup: BeautifulSoup) -> str:
    """
    ê¸°ë³¸ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ (BBC, CNN ë“±)
    - <article> ìˆìœ¼ë©´ ê·¸ ì•ˆì˜ <p>, ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ <p>
    """
    article = soup.find("article") or soup.body
    paragraphs = [
        p.get_text(" ", strip=True) for p in article.find_all("p", recursive=True)
    ]
    paragraphs = [t for t in paragraphs if t]
    return "\n\n".join(paragraphs).strip()


def extract_article_content(url: str) -> str:
    """
    URL ê¸°ì¤€ìœ¼ë¡œ ë„ë©”ì¸ ë¶„ê¸°í•´ì„œ ì ì ˆí•œ íŒŒì„œ ì‚¬ìš©
    (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í˜¹ì‹œ ëª°ë¼ ë³´ì¡´)
    """
    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, "html.parser")
    domain = urlparse(url).netloc.lower()

    if "reuters.com" in domain:
        content = extract_reuters_article(soup)
    else:
        content = extract_generic_article(soup)

    # ë„ˆë¬´ ì§§ìœ¼ë©´ (JS ë¡œë“œ ì•ˆëœ ê²½ìš° ë“±) í•œ ë²ˆ ë” fallback
    if not content or len(content.strip()) < 50:
        meta_og = soup.find("meta", attrs={"property": "og:description"})
        if meta_og and meta_og.get("content"):
            content = meta_og["content"].strip()

    return content


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë¡œì»¬ AI ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ (USE_LOCAL_AI=true ì¼ ë•Œë§Œ ì‚¬ìš©)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

sentiment_analyzer = None

def _get_sentiment_analyzer():
    """
    ê°ì„± ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë“œ ì „ìš© - USE_LOCAL_AI=true)
    í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    """
    global sentiment_analyzer
    if sentiment_analyzer is None:
        try:
            import torch
            from transformers import pipeline
            import gc
            import psutil
            
            process = psutil.Process(os.getpid())
            mem_before = process.memory_info().rss / 1024 / 1024
            
            logger.info("ğŸ”„ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘ (~268MB)...")
            sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="distilbert-base-uncased-finetuned-sst-2-english",
                device=-1,
                framework="pt"
            )
            gc.collect()
            
            mem_after = process.memory_info().rss / 1024 / 1024
            logger.info(f"âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (+{mem_after - mem_before:.1f} MB)")
        except Exception as e:
            logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=503, detail="ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return sentiment_analyzer

# -------------------------------
# 1. BBC RSS ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
# -------------------------------
@router.get("/bbc")
def get_bbc_news():
    try:
        rss_url = "http://feeds.bbci.co.uk/news/world/rss.xml"
        logger.info(f"BBC RSS í”¼ë“œ ìš”ì²­: {rss_url}")
        
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.warning("BBC RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"articles": [], "message": "ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        articles = []
        for entry in feed.entries[:20]:
            try:
                article = {
                    "title": entry.get("title", "ì œëª© ì—†ìŒ"),
                    "link": entry.get("link", ""),
                    "summary": entry.get("summary", entry.get("description", "ìš”ì•½ ì—†ìŒ")),
                    "published": entry.get("published", "")
                }
                articles.append(article)
            except Exception as e:
                logger.warning(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"BBC ë‰´ìŠ¤ {len(articles)}ê°œ ë¡œë”© ì™„ë£Œ")
        return {"articles": articles}
        
    except Exception as e:
        logger.error(f"BBC ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# -------------------------------
# Reuters RSS ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
# -------------------------------
@router.get("/reuters")
def get_reuters_news():
    try:
        # Reuters ê³µì‹ RSS í”¼ë“œ ì‚¬ìš©
        rss_url = "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best"
        logger.info(f"Reuters RSS í”¼ë“œ ìš”ì²­: {rss_url}")
        
        feed = feedparser.parse(rss_url)
        logger.info(f"íŒŒì‹±ëœ entries ê°œìˆ˜: {len(feed.entries)}")
        
        if not feed.entries:
            logger.warning("Reuters RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"articles": [], "message": "ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        articles = []
        for entry in feed.entries[:20]:
            try:
                title = entry.get("title", "ì œëª© ì—†ìŒ")

                # summaryì—ì„œ HTML íƒœê·¸ ì œê±°
                summary = entry.get("summary", entry.get("description", ""))
                if summary:
                    soup = BeautifulSoup(summary, "html.parser")
                    summary = soup.get_text().strip()

                article = {
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": summary if summary else "",
                    "published": entry.get("published", "")
                }
                articles.append(article)
            except Exception as e:
                logger.warning(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"Reuters ë‰´ìŠ¤ {len(articles)}ê°œ ë¡œë”© ì™„ë£Œ")
        return {"articles": articles}
        
    except Exception as e:
        logger.error(f"Reuters ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# -------------------------------
# CNN RSS ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
# -------------------------------
@router.get("/cnn")
def get_cnn_news():
    try:
        # CNN ê³µì‹ RSS í”¼ë“œ ì‚¬ìš© (Top Stories)
        rss_url = "http://rss.cnn.com/rss/edition.rss"
        logger.info(f"CNN RSS í”¼ë“œ ìš”ì²­: {rss_url}")
        
        feed = feedparser.parse(rss_url)
        logger.info(f"íŒŒì‹±ëœ entries ê°œìˆ˜: {len(feed.entries)}")
        
        if not feed.entries:
            logger.warning("CNN RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"articles": [], "message": "ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        articles = []
        for entry in feed.entries[:20]:
            try:
                # Google Newsì—ì„œ ê°€ì ¸ì˜¨ ì œëª© ì •ë¦¬ (- CNN ì œê±°)
                title = entry.get("title", "ì œëª© ì—†ìŒ")
                title = title.split(" - CNN")[0].strip()

                # Google NewsëŠ” summaryì— HTMLì´ í¬í•¨ë  ìˆ˜ ìˆìŒ
                description = entry.get("description", entry.get("summary", ""))
                if description:
                    soup = BeautifulSoup(description, "html.parser")
                    clean_desc = soup.get_text(strip=True)
                else:
                    clean_desc = ""

                article = {
                    "title": title,
                    "link": entry.get("link", ""),
                    "summary": "",  # Google News CNNì€ ìš”ì•½ í‘œì‹œ ì•ˆ í•¨ (Reutersì™€ ë™ì¼)
                    "published": entry.get("published", "")
                }
                articles.append(article)
            except Exception as e:
                logger.warning(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"CNN ë‰´ìŠ¤ {len(articles)}ê°œ ë¡œë”© ì™„ë£Œ")
        return {"articles": articles}
        
    except Exception as e:
        logger.error(f"CNN ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# -------------------------------
# í†µí•© ë‰´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ (ë§¤ì²´ ì„ íƒ)
# -------------------------------
@router.get("/news")
def get_news(source: str = "BBC"):
    """
    ë§¤ì²´ë³„ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    source: BBC, Reuters (ë¡œì´í„°), CNN
    """
    try:
        source_lower = source.lower()
        
        if source_lower == "bbc":
            return get_bbc_news()
        elif source_lower in ["reuters", "reuters (ë¡œì´í„°)", "ë¡œì´í„°"]:
            return get_reuters_news()
        elif source_lower == "cnn":
            return get_cnn_news()
        else:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ì²´ì…ë‹ˆë‹¤: {source}. BBC, Reuters (ë¡œì´í„°), CNN ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ í—¬í¼ (ì‹¤ì œ detail/summaryì—ì„œ ì‚¬ìš©í•˜ëŠ” ë²„ì „)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _extract_article_text(final_url: str, html: str) -> str:
    """
    ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë³„ë¡œ ë³¸ë¬¸ì„ íŒŒì‹±í•˜ëŠ” í—¬í¼.
    - Reuters: ìƒˆ ë ˆì´ì•„ì›ƒ(+êµ¬ ë ˆì´ì•„ì›ƒ) ëª¨ë‘ ì‹œë„
    - ê¸°íƒ€: article íƒœê·¸ â†’ ì „ì²´ p ìˆœìœ¼ë¡œ ì‹œë„
    """
    domain = urlparse(final_url).netloc.lower()
    soup = BeautifulSoup(html, "html.parser")

    paragraphs = []

    if "reuters.com" in domain:
        # 1) ìƒˆ ë ˆì´ì•„ì›ƒ: <div data-testid="Body"> ì•ˆì˜ <p>
        body = soup.find("div", attrs={"data-testid": "Body"})
        if body:
            paragraphs = body.find_all("p")

        # 2) ëª» ì°¾ìœ¼ë©´ <article> ì•ˆì˜ <p>
        if not paragraphs:
            article_tag = soup.find("article")
            if article_tag:
                paragraphs = article_tag.find_all("p")

        # 3) ì•„ì£¼ ì˜›ë‚  ë ˆì´ì•„ì›ƒ: class ê¸°ë°˜
        if not paragraphs:
            old_body = soup.find("div", class_="article-body__content")
            if old_body:
                paragraphs = old_body.find_all("p")
    else:
        # ê¸°íƒ€ ì‚¬ì´íŠ¸: article íƒœê·¸ ìš°ì„ , ì—†ìœ¼ë©´ ì „ì²´ p
        article_tag = soup.find("article")
        if article_tag:
            paragraphs = article_tag.find_all("p")

    # ìµœì¢… fallback: ê·¸ëƒ¥ í˜ì´ì§€ ì „ì²´ p
    if not paragraphs:
        paragraphs = soup.find_all("p")

    texts = [
        p.get_text(" ", strip=True)
        for p in paragraphs
        if p.get_text(" ", strip=True)
    ]
    content = " ".join(texts)

    logger.info(
        f"ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: domain={domain}, paragraphs={len(paragraphs)}, length={len(content)}"
    )

    # ë„ˆë¬´ ì§§ìœ¼ë©´ og:descriptionë„ í•œ ë²ˆ í™•ì¸
    if len(content.strip()) < 20:
        og = soup.find("meta", property="og:description")
        if og and og.get("content"):
            content = og["content"].strip()

    return content

# -------------------------------
# 2. íŠ¹ì • ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
# -------------------------------
@router.get("/detail")
def get_news_detail(url: str):
    try:
        # Google News URLì¸ ê²½ìš° ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
        if "news.google.com" in url:
            # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            res = requests.get(
                url,
                headers={"User-Agent": "Mozilla/5.0"},
                timeout=10,
                allow_redirects=True,
            )

            # HTMLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
            soup = BeautifulSoup(res.text, "html.parser")

            # ë°©ë²• 1: <a> íƒœê·¸ì—ì„œ ì‹¤ì œ ë§í¬ ì°¾ê¸°
            link_tag = soup.find("a", href=True)
            if link_tag and link_tag.get("href"):
                actual_url = link_tag["href"]
                # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLì¸ì§€ í™•ì¸
                if not "google.com" in actual_url:
                    url = actual_url
                    logger.info(f"Google Newsì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ: {url}")

        res = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=10,
            allow_redirects=True,
        )

        final_url = res.url
        html = res.text

        content = _extract_article_text(final_url, html)

        if not content or len(content.strip()) == 0:
            logger.warning("ë‰´ìŠ¤ ë³¸ë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return {
                "url": final_url,
                "content": "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            }

        return {"url": final_url, "content": content[:3000]}
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë‰´ìŠ¤ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}",
        )

# -------------------------------
# 3. ê¸°ì‚¬ ìš”ì•½í•˜ê¸° (Cloud Run AI ì„œë¹„ìŠ¤)
# -------------------------------
@router.get("/summary")
def summarize_news(url: str):
    try:
        logger.info(f"ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­: {url}")

        # Google News URLì¸ ê²½ìš° ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
        if "news.google.com" in url:
            # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            res = requests.get(
                url,
                headers={"User-Agent": "Mozilla/5.0"},
                timeout=10,
                allow_redirects=True,
            )

            # HTMLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
            soup = BeautifulSoup(res.text, "html.parser")

            # ë°©ë²• 1: <a> íƒœê·¸ì—ì„œ ì‹¤ì œ ë§í¬ ì°¾ê¸°
            link_tag = soup.find("a", href=True)
            if link_tag and link_tag.get("href"):
                actual_url = link_tag["href"]
                # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLì¸ì§€ í™•ì¸
                if not "google.com" in actual_url:
                    url = actual_url
                    logger.info(f"Google Newsì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ: {url}")

        res = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=10,
            allow_redirects=True,
        )
        final_url = res.url
        html = res.text

        content = _extract_article_text(final_url, html)
        content = content.strip()

        if not content:
            logger.warning("ë‰´ìŠ¤ ë³¸ë¬¸ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "url": final_url,
                "summary": "ë³¸ë¬¸ì„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            }

        # ì•„ì£¼ ì§§ì€ ê¸°ì‚¬(í•œë‘ ë¬¸ì¥)ëŠ” ê·¸ëŒ€ë¡œ summaryë¡œ ì‚¬ìš©
        if len(content) < 200:
            logger.info("ë³¸ë¬¸ì´ ì§§ì•„ì„œ ê·¸ëŒ€ë¡œ ìš”ì•½ ê²°ê³¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return {"url": final_url, "summary": content}

        # AI_SERVICE_URLì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ Cloud Run ì‚¬ìš©
        AI_SERVICE_URL = os.getenv("AI_SERVICE_URL")

        if AI_SERVICE_URL:
            # Cloud Run AI ì„œë¹„ìŠ¤ë¡œ ìš”ì•½
            logger.info("â˜ï¸  ìš”ì•½: Cloud Run AI ì„œë¹„ìŠ¤ ì‚¬ìš©")
            payload = {
                "text": content[:2048],
                "max_length": 130,
                "min_length": 30,
            }
            result = call_ai_service("/summarize", payload, timeout=120)
            summary_text = result.get("summary", "").strip() or "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.info("ë‰´ìŠ¤ ìš”ì•½ ì™„ë£Œ (Cloud Run AI)")
            return {"url": final_url, "summary": summary_text}

        else:
            # ë¡œì»¬ ê°œë°œ: ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì²« 3ë¬¸ì¥ ì¶”ì¶œ)
            logger.info("ğŸ“ ìš”ì•½: ë¡œì»¬ ëª¨ë“œ (ì²« 3ë¬¸ì¥ ì¶”ì¶œ)")

            # ë¬¸ì¥ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ë²•)
            sentences = []
            for delimiter in ['. ', '! ', '? ']:
                if delimiter in content:
                    parts = content.split(delimiter)
                    for i, part in enumerate(parts[:-1]):
                        sentences.append(part + delimiter.strip())
                    if parts[-1]:
                        sentences.append(parts[-1])
                    break

            if not sentences:
                # ë¬¸ì¥ êµ¬ë¶„ì´ ì•ˆ ë˜ë©´ ì²« 500ìë§Œ ì‚¬ìš©
                summary_text = content[:500] + "..."
            else:
                # ì²« 3ë¬¸ì¥ ì‚¬ìš©
                summary_text = " ".join(sentences[:3])
                if len(summary_text) > 500:
                    summary_text = summary_text[:500] + "..."

            logger.info("ë‰´ìŠ¤ ìš”ì•½ ì™„ë£Œ (ë¡œì»¬ - ì²« 3ë¬¸ì¥)")
            return {"url": final_url, "summary": summary_text}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìš”ì•½ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë‰´ìŠ¤ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
        )

# -------------------------------
# 4. ê°ì„± ë¶„ì„ API (Cloud Run AI ì„œë¹„ìŠ¤)
# -------------------------------
@router.post("/sentiment")
def analyze_sentiment(data: dict):
    """
    í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤ (Cloud Run AI ì„œë¹„ìŠ¤ë¡œ í”„ë¡ì‹œ)
    """
    try:
        text = data.get("text", "")
        if not text or len(text.strip()) < 10:
            return {"sentiment": "neutral", "score": 0.5, "label": "ì¤‘ë¦½"}
        
        USE_LOCAL_AI = os.getenv("USE_LOCAL_AI", "false").lower() == "true"
        
        if USE_LOCAL_AI:
            logger.info("ğŸ  ê°ì„± ë¶„ì„: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©")
            analyzer = _get_sentiment_analyzer()
            result = analyzer(text[:512])[0]
            
            sentiment_map = {
                "POSITIVE": {"sentiment": "positive", "label": "ê¸ì •"},
                "NEGATIVE": {"sentiment": "negative", "label": "ë¶€ì •"}
            }
            sentiment_info = sentiment_map.get(result["label"], {"sentiment": "neutral", "label": "ì¤‘ë¦½"})
            
            return {
                "sentiment": sentiment_info["sentiment"],
                "label": sentiment_info["label"],
                "score": round(result["score"], 2)
            }
        else:
            # AI_SERVICE_URLì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ Cloud Run ì‚¬ìš©
            AI_SERVICE_URL = os.getenv("AI_SERVICE_URL")

            if AI_SERVICE_URL:
                logger.info("â˜ï¸  ê°ì„± ë¶„ì„: Cloud Run AI ì„œë¹„ìŠ¤ ì‚¬ìš©")
                payload = {"text": text}
                result = call_ai_service("/sentiment", payload, timeout=30)
                return result
            else:
                # ë¡œì»¬ ê°œë°œ: ë‰´ìŠ¤ íŠ¹í™” í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„
                logger.info("ğŸ“Š ê°ì„± ë¶„ì„: ë¡œì»¬ ëª¨ë“œ (ë‰´ìŠ¤ íŠ¹í™” í‚¤ì›Œë“œ)")

                text_lower = text.lower()

                # ê¸ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
                positive_keywords = {
                    # ì„±ê³µ/ì„±ì·¨ (ê°€ì¤‘ì¹˜ 3)
                    'success': 3, 'successful': 3, 'achieve': 3, 'achieved': 3,
                    'accomplishment': 3, 'breakthrough': 3, 'triumph': 3,
                    # ìŠ¹ë¦¬/ìš°ìŠ¹ (ê°€ì¤‘ì¹˜ 3)
                    'win': 3, 'wins': 3, 'won': 3, 'victory': 3, 'champion': 3,
                    # ê¸ì •ì  ë³€í™” (ê°€ì¤‘ì¹˜ 2)
                    'progress': 2, 'improve': 2, 'improved': 2, 'improvement': 2,
                    'growth': 2, 'rise': 2, 'rose': 2, 'rising': 2, 'increase': 2,
                    'gain': 2, 'surge': 2, 'boost': 2, 'recover': 2, 'recovery': 2,
                    'deal': 2, 'deals': 2, 'blockbuster': 2, 'record': 2, 'historic': 2,
                    # ê¸ì •ì  í‰ê°€ (ê°€ì¤‘ì¹˜ 2)
                    'excellent': 2, 'outstanding': 2, 'remarkable': 2, 'impressive': 2,
                    'positive': 2, 'optimistic': 2, 'favorable': 2, 'promising': 2,
                    # ì¼ë°˜ ê¸ì • (ê°€ì¤‘ì¹˜ 1)
                    'good': 1, 'great': 1, 'better': 1, 'best': 1, 'wonderful': 1,
                    'fantastic': 1, 'amazing': 1, 'happy': 1, 'pleased': 1, 'hope': 1,
                    'peace': 1, 'celebrate': 1, 'celebration': 1, 'joy': 1, 'love': 1,
                    'support': 1, 'help': 1, 'agreement': 1, 'cooperation': 1,
                    'agree': 1, 'agreed': 1, 'welcome': 1, 'welcomes': 1, 'welcomed': 1,
                    'benefit': 1, 'benefits': 1, 'opportunity': 1, 'opportunities': 1
                }

                # ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
                negative_keywords = {
                    # í­ë ¥/ì¬ë‚œ (ê°€ì¤‘ì¹˜ 3)
                    'kill': 3, 'killed': 3, 'death': 3, 'deaths': 3, 'die': 3, 'died': 3,
                    'attack': 3, 'attacked': 3, 'war': 3, 'bomb': 3, 'bombard': 3,
                    'explosion': 3, 'disaster': 3, 'tragedy': 3, 'crisis': 3,
                    'emergency': 3, 'terror': 3, 'terrorism': 3, 'violence': 3,
                    # ë²”ì£„/ì‚¬ê³  (ê°€ì¤‘ì¹˜ 3)
                    'murder': 3, 'crash': 3, 'accident': 3, 'fire': 3, 'flood': 3,
                    'earthquake': 3, 'storm': 3, 'hurricane': 3, 'deadly': 3,
                    'shooting': 3, 'shot': 3, 'fighting': 3, 'fight': 3,
                    # ì‹¤íŒ¨/ì†ì‹¤ (ê°€ì¤‘ì¹˜ 2)
                    'fail': 2, 'failed': 2, 'failure': 2, 'loss': 2, 'lost': 2,
                    'lose': 2, 'defeat': 2, 'collapse': 2, 'decline': 2, 'fall': 2,
                    'drop': 2, 'decrease': 2, 'cut': 2, 'slash': 2,
                    # ë¶€ì •ì  í‰ê°€ (ê°€ì¤‘ì¹˜ 2)
                    'bad': 2, 'terrible': 2, 'awful': 2, 'worst': 2, 'worse': 2,
                    'poor': 2, 'negative': 2, 'pessimistic': 2, 'concern': 2,
                    'worry': 2, 'fear': 2, 'threat': 2, 'risk': 2, 'danger': 2,
                    # ì¼ë°˜ ë¶€ì • (ê°€ì¤‘ì¹˜ 1)
                    'problem': 1, 'issue': 1, 'difficult': 1, 'challenge': 1,
                    'trouble': 1, 'conflict': 1, 'dispute': 1, 'protest': 1,
                    'angry': 1, 'sad': 1, 'disappointed': 1, 'sorry': 1
                }

                # ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ì ìˆ˜ ê³„ì‚°
                positive_score = sum(weight for word, weight in positive_keywords.items() if word in text_lower)
                negative_score = sum(weight for word, weight in negative_keywords.items() if word in text_lower)

                logger.info(f"ê¸ì • ì ìˆ˜: {positive_score}, ë¶€ì • ì ìˆ˜: {negative_score}")

                # ì ìˆ˜ ì°¨ì´ê°€ 2 ì´ìƒì´ë©´ ëª…í™•í•œ ê°ì„±ìœ¼ë¡œ íŒë‹¨
                if positive_score > negative_score + 1:
                    sentiment = "positive"
                    label = "ê¸ì •"
                    # ì ìˆ˜ ì°¨ì´ì— ë”°ë¼ ì‹ ë¢°ë„ ê³„ì‚°
                    diff = positive_score - negative_score
                    score = min(0.55 + diff * 0.05, 0.95)
                elif negative_score > positive_score + 1:
                    sentiment = "negative"
                    label = "ë¶€ì •"
                    diff = negative_score - positive_score
                    score = min(0.55 + diff * 0.05, 0.95)
                else:
                    sentiment = "neutral"
                    label = "ì¤‘ë¦½"
                    score = 0.5

                return {
                    "sentiment": sentiment,
                    "label": label,
                    "score": round(score, 2)
                }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ ë°˜í™˜ (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
        logger.warning("ê°ì„± ë¶„ì„ ì‹¤íŒ¨ - ì¤‘ë¦½ ë°˜í™˜")
        return {"sentiment": "neutral", "label": "ì¤‘ë¦½", "score": 0.5}

# -------------------------------
# 5. ìœ ì‚¬ ê¸°ì‚¬ ë¶„ì„ API
# -------------------------------
@router.post("/similarity")
def find_similar_articles(data: dict):
    """
    ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    """
    try:
        target = data.get("target_article", {})
        articles = data.get("articles", [])
        
        if not target or not articles:
            return []
        
        target_text = f"{target.get('title', '')} {target.get('summary', '')}"
        article_texts = [f"{art.get('title', '')} {art.get('summary', '')}" for art in articles]
        
        if not target_text.strip():
            return []
        
        all_texts = [target_text] + article_texts
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        target_vector = tfidf_matrix[0:1]
        article_vectors = tfidf_matrix[1:]
        similarities = cosine_similarity(target_vector, article_vectors)[0]
        
        similar_articles = []
        for idx, sim in enumerate(similarities):
            if sim >= 0.3:
                similar_articles.append({
                    "index": idx,
                    "similarity": round(float(sim), 2),
                    "title": articles[idx].get("title", ""),
                    "url": articles[idx].get("url", "")
                })
        
        similar_articles.sort(key=lambda x: x["similarity"], reverse=True)
        return similar_articles[:5]
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ìœ ì‚¬ ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# -------------------------------
# 6. ì¶”ì²œ ë‰´ìŠ¤ API (TOP 10 ì¤‘ í•„í„°ë§)
# -------------------------------
class RecommendRequest(BaseModel):
    user_id: int
    topic: Optional[str] = None  # ì‚¬ìš©ì ê´€ì‹¬ ì£¼ì œ (ì •ì¹˜, ê²½ì œ, ê¸°ìˆ , ìŠ¤í¬ì¸ , ë¬¸í™”)
    articles: List[Dict[str, Any]]  # TOP 10 ë‰´ìŠ¤ ëª©ë¡

@router.post("/recommend")
def recommend_news(data: RecommendRequest, db: Session = Depends(get_db)):
    """
    ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ ë‰´ìŠ¤ 5ê°œ ë°˜í™˜ (ê´€ì‹¬ì‚¬ ê¸°ë°˜ 2ê°œ + ì¸ê¸° ë‰´ìŠ¤ 3ê°œ)
    """
    try:
        user_id = data.user_id
        topic = data.topic
        articles = data.articles
        
        if not articles or len(articles) == 0:
            return {"recommended": []}
        
        topic_keywords = {
            "ì •ì¹˜": ["election", "government", "politics", "minister", "president", "policy", "vote", "parliament", "congress"],
            "ê²½ì œ": ["economy", "market", "finance", "stock", "trade", "business", "investment", "inflation", "GDP", "bank"],
            "ê¸°ìˆ ": ["technology", "AI", "software", "digital", "innovation", "tech", "smartphone", "computer", "cyber", "robot", "space"],
            "ìŠ¤í¬ì¸ ": ["sports", "football", "soccer", "basketball", "olympic", "game", "player", "team", "match", "tournament"],
            "ë¬¸í™”": ["culture", "movie", "music", "art", "entertainment", "film", "celebrity", "festival", "book", "theater"]
        }
        
        interest_based_articles = []
        if topic and topic in topic_keywords:
            keywords = topic_keywords[topic]
            
            scored_for_interest = []
            for article in articles:
                title = article.get("title", "").lower()
                summary = article.get("summary", "").lower()
                content = f"{title} {summary}"
                
                matched_keywords = sum(1 for keyword in keywords if keyword.lower() in content)
                interest_score = matched_keywords / len(keywords) if len(keywords) > 0 else 0.0
                
                if interest_score > 0:
                    scored_for_interest.append({
                        "article": article,
                        "score": interest_score
                    })
            
            if scored_for_interest:
                scored_for_interest.sort(key=lambda x: x["score"], reverse=True)
                interest_based_articles = [
                    {
                        **item["article"],
                        "recommendation_reason": "interest"
                    }
                    for item in scored_for_interest[:2]
                ]
        
        if not interest_based_articles:
            interest_based_articles = [
                {
                    **articles[0],
                    "recommendation_reason": "interest"
                },
                {
                    **articles[1],
                    "recommendation_reason": "interest"
                } if len(articles) > 1 else None
            ]
            interest_based_articles = [a for a in interest_based_articles if a is not None]
        
        selected_urls = {article.get("url") for article in interest_based_articles}
        
        popular_candidates = []
        for article in articles:
            if article.get("url") in selected_urls:
                continue
            
            score = 0.0
            
            published_str = article.get("published", "")
            if published_str:
                try:
                    published_date = date_parser.parse(published_str)
                    now = datetime.now(published_date.tzinfo) if published_date.tzinfo else datetime.now()
                    time_diff_hours = (now - published_date).total_seconds() / 3600
                    
                    if time_diff_hours < 24:
                        score += 10.0
                    elif time_diff_hours < 48:
                        score += 5.0
                    elif time_diff_hours < 72:
                        score += 2.0
                    else:
                        score += 1.0
                except:
                    score += 1.0
            else:
                score += 1.0
            
            sentiment = article.get("sentiment", "neutral")
            if sentiment == "positive":
                score += 5.0
            elif sentiment == "neutral":
                score += 2.0
            
            popular_candidates.append({
                "article": article,
                "score": score
            })
        
        popular_candidates.sort(key=lambda x: x["score"], reverse=True)
        popular_articles = [
            {
                **item["article"],
                "recommendation_reason": "trending"
            }
            for item in popular_candidates[:3]
        ]
        
        recommended = interest_based_articles + popular_articles
        
        seen_urls = set()
        unique_recommended = []
        for item in recommended:
            url = item.get("url")
            if url not in seen_urls:
                unique_recommended.append(item)
                seen_urls.add(url)
        
        final_recommended = unique_recommended[:5]
        
        logger.info(f"ì¶”ì²œ ë‰´ìŠ¤ {len(final_recommended)}ê°œ ìƒì„± ì™„ë£Œ (user_id={user_id}, topic={topic})")
        logger.info(f"  - ê´€ì‹¬ì‚¬ ê¸°ë°˜: {len(interest_based_articles)}ê°œ")
        logger.info(f"  - ì¸ê¸° ë‰´ìŠ¤: {len(popular_articles)}ê°œ")
        
        return {
            "recommended": final_recommended,
            "total": len(final_recommended)
        }
        
    except Exception as e:
        logger.error(f"ì¶”ì²œ ë‰´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì¶”ì²œ ë‰´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
